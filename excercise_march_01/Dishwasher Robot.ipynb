{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dishwasher robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.9, 12)\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "sns.set(font_scale=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we will write the transition and reward matrices and dictionaries that map actions as numbers to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = {'grab':0, 'dry': 1, 'store':2}\n",
    "states = {'no plate held': 0, 'wet plate held': 1, 'dry plate held':2, 'finished':3}\n",
    "\n",
    "actions_r = {0: 'grab', 1: 'dry', 2:'store'}\n",
    "states_r = {0: 'no plate held', 1: 'wet plate held', 2: 'dry plate held', 3:'finished'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = 0.10 \n",
    "P0 = np.array(((0.0, 1.0, 0.0, 0.0), (0.0, 1.0, 0.0, 0.0), (0.0, 1.0, 0.0, 0.0), (0.0, 0.0, 0.0, 1.0)))\n",
    "P1 = np.array(((1.0, 0.0, 0.0, 0.0), (0.0, pf, 1-pf, 0.0), (0.0, pf, 1-pf, 0.0), (0.0, 0.0, 0.0, 1.0)))\n",
    "P2 = np.array(((0.0, 0.0, 0.0, 1.0), (0.0, 0.0, 0.0, 1.0), (0.0, 0.0, 0.0, 1.0), (0.0, 0.0, 0.0, 1.0)))\n",
    "\n",
    "P = np.array((P0, P1, P2))\n",
    "\n",
    "# The probability of having a wet plate when you grab a plate\n",
    "P[actions['grab'], states['no plate held'], states['wet plate held']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "R = np.array(((0, 0, 0), (-20, pf*(-20.0), 5), (-20, pf*(-20.0), 10), (0, 0, 0)))\n",
    "\n",
    "# The expected reaward of storing a plate when we are helding a dry plate \n",
    "print(R[states['dry plate held'], actions['store']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policiy evaluation\n",
    "#### Dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array(((1, 0, 0), (1, 0, 0), (1, 0, 0), (1, 0, 0)))\n",
    "\n",
    "pi[states['wet plate held']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_evalution_step(V, gamma, states, actions, pi, P, R):\n",
    "\n",
    "    v = np.zeros_like(V)\n",
    "    \n",
    "    for s in states.values():\n",
    "        value_aux = 0\n",
    "        for a_ in actions.values():\n",
    "            auxs = 0\n",
    "            for s_ in states.values():\n",
    "                auxs += P[a_, s, s_] * (R[s, a_] + gamma * V[s_])\n",
    "\n",
    "            value_aux += pi[s, a_] * auxs \n",
    "\n",
    "        v[s] = value_aux\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0. -20. -20.   0.]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros(len(states))\n",
    "gamma = 1.0\n",
    "V = policy_evalution_step(V, gamma, states, actions, pi, P, R)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with the results from the class. If we doo policy iteration more, we see that the values of the states where we have a plate (both wet and dry) keep getting worse, that is, because every time we grab a new plate over there we break the old and pay a high price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-20. -40. -40.   0.]\n",
      "[-40. -60. -60.   0.]\n",
      "[-60. -80. -80.   0.]\n"
     ]
    }
   ],
   "source": [
    "V = policy_evalution_step(V, gamma, states, actions, pi, P, R)\n",
    "print(V)\n",
    "\n",
    "V = policy_evalution_step(V, gamma, states, actions, pi, P, R)\n",
    "print(V)\n",
    "\n",
    "V = policy_evalution_step(V, gamma, states, actions, pi, P, R)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy impovement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_Q(V, gamma, states, actions, pi, P, R):\n",
    "\n",
    "    Q = np.zeros((len(states), len(actions)))\n",
    "    for s in states.values():\n",
    "        for a_ in actions.values():\n",
    "            auxs = 0\n",
    "            for s_ in states.values():\n",
    "                auxs += P[a_, s, s_] * (R[s, a_] + gamma * V[s_])\n",
    "\n",
    "            Q[s, a_] = auxs\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0. -20. -20.   0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [-20.,  -2.,   5.],\n",
       "       [-20.,  -2.,  10.],\n",
       "       [  0.,   0.,   0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.zeros(len(states))\n",
    "gamma = 1.0\n",
    "V = policy_evalution_step(V, gamma, states, actions, pi, P, R)\n",
    "print(V)\n",
    "V = np.zeros(len(states))\n",
    "Q = calculate_Q(V, gamma, states, actions, pi, P, R)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dry plate held': 2, 'finished': 3, 'no plate held': 0, 'wet plate held': 1}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dry': 1, 'grab': 0, 'store': 2}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner = np.argmax(Q, axis=1)\n",
    "pi_new = np.zeros((len(states), len(actions)))\n",
    "for index, win in enumerate(winner):\n",
    "    pi_new[index, win] = 1  \n",
    "\n",
    "pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V [  0.   5.  10.   0.]\n",
      "Q [[  5.    0.    0. ]\n",
      " [-15.    7.5   5. ]\n",
      " [-15.    7.5  10. ]\n",
      " [  0.    0.    0. ]]\n"
     ]
    }
   ],
   "source": [
    "V = policy_evalution_step(V, gamma, states, actions, pi_new, P, R)\n",
    "Q = calculate_Q(V, gamma, states, actions, pi_new, P, R)\n",
    "print('V', V)\n",
    "print('Q', Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner = np.argmax(Q, axis=1)\n",
    "pi_new = np.zeros((len(states), len(actions)))\n",
    "for index, win in enumerate(winner):\n",
    "    pi_new[index, win] = 1  \n",
    "\n",
    "pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V [  5.    7.5  10.    0. ]\n",
      "Q [[  7.5    5.     0.  ]\n",
      " [-12.5    7.75   5.  ]\n",
      " [-12.5    7.75  10.  ]\n",
      " [  0.     0.     0.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = policy_evalution_step(V, gamma, states, actions, pi_new, P, R)\n",
    "Q = calculate_Q(V, gamma, states, actions, pi_new, P, R)\n",
    "print('V', V)\n",
    "print('Q', Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner = np.argmax(Q, axis=1)\n",
    "pi_new = np.zeros((len(states), len(actions)))\n",
    "for index, win in enumerate(winner):\n",
    "    pi_new[index, win] = 1  \n",
    "\n",
    "pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "pi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dry': 1, 'grab': 0, 'store': 2}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
